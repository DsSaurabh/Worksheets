                                                  MACHINE LEARNING
                                                 
                                                   WORKSHEET – 1

In Q1 to Q7, only one option is correct, Choose the correct option:

1. The value of correlation coefficient will always be:
C) between -1 and 1

2. Which of the following cannot be used for dimensionality reduction?
C) Recursive feature elimination

3. Which of the following is not a kernel in Support Vector Machines?
C) hyperplane

4. Amongst the following, which one is least suitable for a dataset having non-linear decision boundaries?
D) Support Vector Classifier

5. In a Linear Regression problem, ‘X’ is independent variable and ‘Y’ is dependent variable, where ‘X’ represents weight in pounds. If you convert the unit of ‘X’ 
to kilograms, then new coefficient of ‘X’ will be?
(1 kilogram = 2.205 pounds)
C) old coefficient of ‘X’ ÷ 2.205

6. As we increase the number of estimators in ADABOOST Classifier, what happens to the accuracy of the model?
B) increases

7. Which of the following is not an advantage of using random forest instead of decision trees?
C) Random Forests are easy to interpret

In Q8 to Q10, more than one options are correct, Choose all the correct options:

8. Which of the following are correct about Principal Components?
B) Principal Components are calculated using unsupervised learning techniques
C) Principal Components are linear combinations of Linear Variables.

9. Which of the following are applications of clustering?
A) Identifying developed, developing and under-developed countries on the basis of factors like GDP, poverty index, employment rate, population and living index
B) Identifying loan defaulters in a bank on the basis of previous years’ data of loan accounts.

10. Which of the following is(are) hyper parameters of a decision tree?
A) max_depth 
B) max_features
D) min_samples_leaf

Q10 to Q15 are subjective answer type questions, Answer them briefly.

11. What are outliers? Explain the Inter Quartile Range(IQR) method for outlier detection.
Answer)Outliers:--> An oitliers is a data point that is distant from other similar points. They may be due to variability in the measurement or may indicate 
                    experimental errors.

IQR(Inter Quartile Range)--> The concept of IQR is used to build the boxplot graphs. IQR is concept in statistics that is used to measure the statistical dispersion
                             and data variability by dividing the data set into quartile.
                             It is used to define the outliers.
IQR is the difference between the third quartile and first quartile.
         IQR = Q3-Q1   where Q3 is third quartile and Q1 is first quartile.
Outliers in this case are define as the observations that are below (Q1-1.5*IQR) or above (Q3+1.5*IQR) are treated as outliers.

12. What is the primary difference between bagging and boosting algorithms?
Answer) Bagging Algorithm---> Bagging stands for bootstrap aggregation. It combines multiple learners in a way to reduce the variance of estimates.
                              Random Forest trains N decision tree, you train N different trees on different random subsets of the data and perform voting for
                              final prediction. Bagging technique  reduce the variance of a model to prevent over-fitting and increases the accuracy of unstable
                              model.
Bagging ensembles methods are Random Forest and Extra Trees.

Boosting Algorithm---> Boosting algorithm are aset of low accurate classifer to create a highly accurate classifier. Boosting enables us to implement a strong model
                       by combining a number of weak models together. It decreases the bias and variance when comapre to bagging models.
Boosting ensembles methods are AdaBoost and Gradient Boosting.

Differences Between Bagging and Boosting-–->

  S.NO                                           BAGGING	                                   BOOSTING
1.	    Simplest way of combining predictions that belong to the same type.	             A way of combining predictions that belong to the different types.
2.	    Aim to decrease variance, not bias.	                                             Aim to decrease bias, not variance.
3.	    Each model receives equal weight.	                                             Models are weighted according to their performance.
4.	    Each model is built independently.	                                             New models are influenced by performance of previously built models.
5.	    Different training data subsets are randomly drawn with replacement              Every new subsets contains the elements that were misclassified by
            from the entire training dataset.                                                previous models.
6.	    Bagging tries to solve over-fitting problem.	                             Boosting tries to reduce bias.
7.	    If the classifier is unstable (high variance), then apply bagging.	             If the classifier is stable and simple (high bias) the apply boosting.
8.	    Random forest.	                                                             Gradient boosting.

13. What is adjusted R2 in logistic regression. How is it calculated?
Answer)R2 score shows how well our data points fit a curve or line. Adjusted R2 also indicated how well data points fit a curve or line, but adjusts for the number
       of data points in a model. If you add more and more useless variables to a model, adjusted r-squared will decrease. If you add more useful variables, 
       adjusted r-squared will increase.
   
   R2adj= 1-[(1-R2)(n-1)/(n-k-1)]
   where N is the number of points in your data sample.
   k is the number of independent regressors,i.e the number of variables in your model,excluding the constants.

Adjusted R2 will always be less than or equal to R2.

14. What is the difference between standardisation and normalisation?
Answer) Standardisation---> It transform data to have a mean of zero and standard deviation of 1.
        Normalization----> It usually means to scale a variable to have a values between 0 and 1.

15. What is cross-validation? Describe one advantage and one disadvantage of using cross-validation?
Answer) Cross Validation is a technique for evaluating machine learning models on subsets of the available input data and evaluating them on the complementary 
        subset of the data. 
       In cross validation we partitioning our data into k parts. It holds one of the parts aside as "test" set and then fits the model on the remaining k-1 parts,
       the "training" set. The fitted model is then used on the test and an error rate is calculated. This process is repeated untill all k parts have been used as
       a "test" set. The final error of the model is some average across all the models.

Advantage----->Reduce Overfitting.
               Hyperparameter tunning.

Disadvantage--->Increasestraining time.
                Needs expensive computation.